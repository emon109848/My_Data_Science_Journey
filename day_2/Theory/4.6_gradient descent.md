
---

# üìå GRADIENT DESCENT ‚Äî CHEAT SHEET

---

## 1Ô∏è‚É£ What is Gradient Descent?

* **Purpose:** Minimize a **loss function** \(L(\theta)\) by updating parameters \(\theta\) iteratively
* **Gradient \((\nabla L)\)** ‚Üí slope / direction of steepest increase
* Update moves **opposite to gradient** ‚Üí towards minimum

---

## 2Ô∏è‚É£ Components

1. **Loss Function \(L(\theta)\)**

   * Measures error between predicted and true values
   * Example: Mean Squared Error (MSE)

\[
L(\theta) = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2
\]

2. **Gradient**

   * Partial derivatives w.r.t parameters
   * Shows **direction to increase loss**

3. **Learning Rate \((\alpha)\)**

   * Step size for each update
   * Too high ‚Üí may **overshoot** minimum
   * Too low ‚Üí **slow convergence**

4. **Update Rule**

\[
\theta := \theta - \alpha \nabla L(\theta)
\]

* Move **opposite to gradient** to reduce loss

---

## 3Ô∏è‚É£ NumPy Example (Simple Linear Regression)

```python
import numpy as np

# Data
X = np.array([1,2,3,4])
y = np.array([2,4,6,8])

theta = 0  # initial weight
lr = 0.01  # learning rate
epochs = 1000

for _ in range(epochs):
    y_pred = theta * X
    grad = (-2/len(X)) * np.sum(X * (y - y_pred))
    theta = theta - lr * grad

print("Learned theta:", theta)  # ~2
```

* **Theta converges** to the optimal weight
* Works for **any differentiable loss function**

---

## 4Ô∏è‚É£ Practical Use Cases

1. **Machine Learning Models**

   * Linear regression, logistic regression, neural networks

2. **Optimization Problems**

   * Minimizing cost functions in engineering, economics

3. **Deep Learning / Backpropagation**

   * Neural network weights updated via gradient descent

4. **Reinforcement Learning**

   * Policy/value function optimization

---

## 5Ô∏è‚É£ Quick Tips

* **Batch Gradient Descent:** Full dataset ‚Üí stable but slower

* **Stochastic Gradient Descent:** Single sample ‚Üí noisy, faster

* **Mini-batch Gradient Descent:** Compromise ‚Üí widely used in DL

* Monitor **loss over iterations** ‚Üí check convergence

---

## üî• Interview Questions

1. What is the purpose of gradient descent?
2. Explain gradient, learning rate, and update rule.
3. Difference between batch, stochastic, and mini-batch gradient descent.
4. What happens if learning rate is too high or too low?
5. Show a simple NumPy implementation for linear regression.
6. Give a real-world ML example where gradient descent is used.
7. How does gradient descent relate to backpropagation in neural networks?

---

If you want, I can **next make a fully integrated ‚ÄúNumPy Linear Algebra + Gradient Descent Practical Cheat Sheet‚Äù**, combining:

* Dot product, transpose, inverse, determinant
* Eigenvectors, vector normalization, cosine similarity
* Gradient descent & updates

All with **practical examples + interview questions**, perfect for Day 2 revision.

Do you want me to do that next?
